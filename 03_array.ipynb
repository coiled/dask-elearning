{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/dask/dask/main/docs/source/images/dask_horizontal.svg\"\n",
    "     width=\"60%\"\n",
    "     alt=\"Dask logo\\\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask array provides a parallel, larger-than-memory implementation of NumPy. \n",
    "\n",
    "It will look and feel a lot like NumPy, but does not suffer from the same scalability limitations.\n",
    "\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/latest/_images/dask-array.svg\" width=\"50%\" align=\"center\" alt=\"Dask array\">\n",
    "\n",
    "\n",
    "*  **Parallel**: Uses all of the cores on your computer\n",
    "*  **Larger-than-memory**:  Lets you work on datasets that are larger than your available memory by breaking up your array into many small pieces, operating on those pieces in an order that minimizes the memory footprint of your computation, and effectively streaming data from disk.\n",
    "*  **Blocked Algorithms**:  Perform large computations by performing many smaller computations\n",
    "\n",
    "In this notebook, we'll build some understanding by implementing some blocked algorithms from scratch. We'll then use Dask Array to analyze large datasets, in parallel, using a familiar NumPy-like API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocked Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *blocked algorithm* executes on a large dataset by breaking it up into many small blocks.\n",
    "\n",
    "For example, consider taking the sum of a billion numbers.  We might instead break up the array into 1,000 chunks, each of size 1,000,000, take the sum of each chunk, and then take the sum of the intermediate sums.\n",
    "\n",
    "We achieve the intended result (one sum on one billion numbers) by performing many smaller results (one thousand sums on one million numbers each, followed by another sum of a thousand numbers.)\n",
    "\n",
    "We do exactly this with Python and NumPy in the following example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run prep-alt.py -d random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with h5py\n",
    "# this creates a pointer to the data, but does not actually load\n",
    "import os\n",
    "\n",
    "import h5py\n",
    "\n",
    "f = h5py.File(os.path.join(\"data\", \"random.hdf5\"), mode=\"r\")\n",
    "dset = f[\"/x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Compute sum using blocked algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using dask, let's consider the concept of blocked algorithms. We can compute the sum of a large number of elements by loading them chunk-by-chunk, and keeping a running total.\n",
    "\n",
    "Here we compute the sum of this large array on disk by \n",
    "\n",
    "1.  Computing the sum of each 1,000,000 sized chunk of the array\n",
    "2.  Computing the sum of the 1,000 intermediate sums\n",
    "\n",
    "Note that this is a sequential process in the notebook kernel, both the loading and summing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute sum of large array, one million numbers at a time\n",
    "sums = []\n",
    "for i in range(0, 1_000_000_000, 1_000_000):\n",
    "    chunk = dset[i : i + 1_000_000]  # pull out numpy array\n",
    "    sums.append(chunk.sum())\n",
    "\n",
    "total = sum(sums)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dask.array` contains these algorithms\n",
    "--------------------------------------------\n",
    "\n",
    "Dask.array is a NumPy-like library that does these kinds of tricks to operate on large datasets that don't fit into memory.  It extends beyond the linear problems discussed above to full N-Dimensional algorithms and a decent subset of the NumPy interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create `dask.array` object**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a `dask.array` `Array` object with the `da.from_array` function.  This function accepts\n",
    "\n",
    "1.  `data`: Any object that supports NumPy slicing, like `dset`\n",
    "2.  `chunks`: A chunk size to tell us how to block up our array, like `(1_000_000,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "x = da.from_array(dset, chunks=(1_000_000,))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manipulate `dask.array` object as you would a numpy array**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an `Array` we perform standard numpy-style computations like arithmetic, mathematics, slicing, reductions, etc..\n",
    "\n",
    "The interface is familiar, but the actual work is different. `dask_array.sum()` builds an expression of the computation. It does not do the computation yet. `numpy_array.sum()` computes the sum immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = x.sum()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask.array objects are lazily evaluated.  Operations like `.sum` build up a graph of blocked tasks to execute.  \n",
    "\n",
    "We ask for the final result with a call to `.compute()`.  This triggers the actual computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:  Compute the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a small change to the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this match your result from before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:  Compute the standard deviation\n",
    "\n",
    "Again, this follows regular NumPy syntax, except for the added `.compute()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.std().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:  Meteorological data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run prep-alt.py -d weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 2GB of weather data in HDF5 files in `data/weather-big/*.hdf5`.  We'll use the `h5py` library to interact with this data and `dask.array` to compute on it.\n",
    "\n",
    "Our goal is to visualize the average temperature on the surface of the Earth for this month.  This will require a mean over all of this data.  We'll do this in the following steps\n",
    "\n",
    "1.  Create `h5py.Dataset` objects for each of the days of data on disk (`dsets`)\n",
    "2.  Wrap these with `da.from_array` calls \n",
    "3.  Stack these datasets along time with a call to `da.stack`\n",
    "4.  Compute the mean along the newly stacked time axis with the `.mean()` method\n",
    "5.  Visualize the result with `matplotlib.pyplot.imshow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import h5py\n",
    "\n",
    "filenames = sorted(glob(os.path.join(\"data\", \"weather-big\", \"*.hdf5\")))\n",
    "dsets = [h5py.File(filename, mode=\"r\")[\"/t2m\"] for filename in filenames]\n",
    "dsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Integrate with `dask.array`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a list of `dask.array` objects out of your list of `h5py.Dataset` objects using the `da.from_array` function with a chunk size of `(500, 500)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the cell below to see the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "arrays = [da.from_array(dset, chunks=(500, 500)) for dset in dsets]\n",
    "arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Stack list of `dask.array` objects into a single `dask.array` object with `da.stack`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack these along the first axis so that the shape of the resulting array is `(31, 5760, 11520)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = da.stack(arrays, axis=0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Plot the mean of this array along the time (`0th`) axis\n",
    "\n",
    "Complete the following:\n",
    "\n",
    "```python\n",
    "result = ...\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "plt.imshow(result, cmap='RdBu_r')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "result = x.mean(axis=0)\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "plt.imshow(result, cmap=\"RdBu_r\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance comparison\n",
    "---------------------------\n",
    "\n",
    "The following experiment was performed on a personal laptop with 16GB of RAM and 8 CPU cores.  Your performance may vary. If you attempt the NumPy version then please ensure that you have more than 4GB of main memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NumPy: ~7s, Needs gigabytes of memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "\n",
    "%%time \n",
    "x = np.random.normal(10, 0.1, size=(20000, 20000)) \n",
    "y = x.mean(axis=0)[::100] \n",
    "y \n",
    "\n",
    "CPU times: user 6.73 s, sys: 331 ms, total: 7.16 s\n",
    "Wall time: 7.11 s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dask Array: ~1.5s, Needs megabytes of memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import dask.array as da\n",
    "\n",
    "%%time\n",
    "x = da.random.normal(10, 0.1, size=(20000, 20000), chunks=(1000, 1000))\n",
    "y = x.mean(axis=0)[::100] \n",
    "y.compute() \n",
    "\n",
    "CPU times: user 635 ms, sys: 119 ms, total: 754 ms\n",
    "Wall time: 1.69 s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask finished faster, but used more total CPU time because Dask was able to transparently parallelize the computation because of the chunk size."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
